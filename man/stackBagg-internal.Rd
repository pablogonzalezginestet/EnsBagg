% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ipcw_bagging.R, R/loss_functions.R,
%   R/parametersSimulation.R, R/utils.R
\docType{data}
\name{ipcw_ensbagg}
\alias{ipcw_ensbagg}
\alias{ipcw_genbagg}
\alias{ipcw_brier}
\alias{ipcw_crossentropy}
\alias{tune_lasso}
\alias{stackBagg-internal}
\alias{optimun_auc_coef}
\alias{risk_auc}
\alias{MLprocedures}
\alias{ML_list}
\alias{MLprocedures_natively}
\alias{ML_list_natively}
\alias{grid_parametersDataHIV}
\title{Algorithm 2:  Procedure to obtain optimally the coefficients to be used in Algorithm 1}
\format{An object of class \code{list} of length 8.}
\usage{
ipcw_ensbagg(folds, MLprocedures, fmla, tuneparams, tao, B = NULL, A,
  data, xnam, xnam.factor, xnam.cont, xnam.cont.gam, ens.library)

ipcw_genbagg(fmla, tuneparams, MLprocedures, traindata, testdata, B, A,
  xnam, xnam.factor, xnam.cont, xnam.cont.gam, ens.library)

ipcw_brier(par, Z, y, wts)

ipcw_crossentropy(par, Z, y, wts)

tune_lasso(folds, fmla, tao, data, xnam)

optimun_auc_coef(coef_init, lambda, data, Z, tao)

risk_auc(par, lambda, Z, data, tao)

MLprocedures(traindata, testdata, fmla, xnam, xnam.factor, xnam.cont,
  xnam.cont.gam, tuneparams, ens.library, i)

ML_list

MLprocedures_natively(traindata, testdata, fmla, xnam, xnam.factor,
  xnam.cont, xnam.cont.gam, tuneparams)

ML_list_natively

grid_parametersDataHIV(xnam, data, tao)
}
\arguments{
\item{folds}{Number of folds}

\item{MLprocedures}{\link{MLprocedures}}

\item{fmla}{formula object ex. "E ~ x1+x2"}

\item{tuneparams}{a list of tune parameters for each machine learning procedure}

\item{tao}{time point of interest}

\item{B}{number of bootstrap samples}

\item{data}{a training data set}

\item{xnam}{all covariates in the model}

\item{xnam.factor}{categorical variables include in the model}

\item{xnam.cont}{continous variables include in the model}

\item{xnam.cont.gam}{continous variables to be included in the smoothing operator gam::s(,df)}

\item{ens.library}{algorithms in the library}

\item{traindata}{a training data set}

\item{testdata}{a test data set}

\item{par}{a vector of weights. Its length must be equal to the number of predictions included in Z}

\item{Z}{a matrix that contains the predictions. Each column represents a single marker.}

\item{y}{vector of response variable (binary).}

\item{wts}{IPC weights}

\item{coef_init}{starting values for the coefficients}

\item{lambda}{penalization term. It is a positive scalar.}

\item{i}{sample selected by bootstrap}

\item{fmla}{formula object ex. "E ~ x1+x2"}

\item{tuneparams}{a list of tune parameters for each machine learning procedure}

\item{MLprocedures}{\link{MLprocedures}}

\item{B}{number of bootstrap samples}

\item{xnam}{all covariates in the model}

\item{xnam.factor}{categorical variables include in the model}

\item{xnam.cont}{continous variables include in the model}

\item{xnam.cont.gam}{continous variables to be included in the smoothing operator gam::s(,df=)}

\item{ens.library}{algorithms in the library}

\item{par}{a vector of weights. Its length must be equal to the number of predictions included in Z}

\item{Z}{a matrix that contains the predictions. Each column represents a single marker.}

\item{y}{vector of response variable (binary).}

\item{wts}{IPC weights}

\item{folds}{number of folds}

\item{fmla}{formula object ex. "E ~ x1+x2"}

\item{tao}{time point of interest}

\item{data}{a training data set}

\item{data}{A data frame that contains at least: ttilde, delta, wts}

\item{Z}{a matrix that contains the predictions. Each column represents a single marker.}

\item{tao}{time point of interest}

\item{par}{a vector of coefficients/weights. Its length must be equal to the number of predictions included in Z}

\item{lambda}{penalization term. It is a positive scalar.}

\item{Z}{a matrix that contains the predictions. Each column represents a single marker.}

\item{data}{A data frame  that constains at least: ttilde= time to event, delta=event type, wts= IPC weights}

\item{tao}{time point of interest}

\item{traindata}{training data set}

\item{testdata}{validation/test data set}

\item{fmla}{formula object ex. "E ~ x1+x2"}

\item{tuneparams}{a list of tune parameters for each machine learning procedure}

\item{traindata}{training data set}

\item{testdata}{validation/test data set}

\item{fmla}{formula object ex. "E ~ x1+x2"}

\item{tuneparams}{a list of tune parameters for each machine learning procedure}

\item{xnam}{a vector with the covariates names considered in the modeling}

\item{data}{a training data set}

\item{tao}{time point of interest}
}
\value{
a list with the predictions of each machine learning algorithm (id, predictions), the average AUC across folds for each of them, the optimal coefficients, an indicator if the optimization procedure has converged and the value of penalization term chosen

a matrix with the predictions on the test data set of each machine learning algorithm considered in \link{MLprocedures}

lambda to be used in the glmnet function

a vector with the optimal AUC value and the optimal coefficient

1-AUC

a matrix of predictions where each column is the prediction of each algorithm based on the testdata

a list of Machine Learning functions

a matrix of predictions where each column is the prediction of each algorithm based on the testdata

a list of Machine Learning functions

a list with a grid of values for each hyperparameter
 gam_param a vector containing degree of freedom 3 and 4
 lasso_param a grid of values for the shrinkage term lambda
 randomforest_param a two column matrix: first column denotes the num_trees parameter and the second column denotes the mtry parameter.
 knn_param a grid of  positive integers values
 svm_param a three column matrix:  first column denotes the cost parameter, second column the gamma and third column the kernel. kernel=1 denotes "radial" and kernel=2 denotes "linear".
nn_param a grid of positive integers values for the neurons
 bart_param a three column matrix:  first column denotes the num_tree parameter, second column the k parameter and third column the q parameter.
}
\description{
Obtain predictions

Obtain predictions

Compute weighted Brier Loss function for a single marker or a linear weighted combination of markers

Compute weighted Cross Entropy Loss function for a single marker or a linear weighted combination of markers

Obtain the lambda hyparameter for the LASSO using cross-validation

Internal stackBagg helper functions

Compute the risk of missclassifying an individual using as a marker a single prediction or weighted linear combination of several predictions (1-AUC)

Predictions based on a library of Machine Learning procedures

Library of Machine Learning procedures

Predictions based on those Machine Learning procedures in the library that allow for weights to be specified as an argument of the R function. No bagging occurs. This group of algorithms is denoted as  Native Weights

Library of Machine Learning procedures that allows for weights

A grid of values for hyperparameters used in the Real Data Application: InfCareHIV Register. This grid of values isan argument in the tuning parameter function tune_parameter_ml.R
}
\details{
These functions are not intended for use by users.
}
\keyword{datasets}
